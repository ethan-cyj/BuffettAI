{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 439\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../preprocess/combined_finetuning_df.csv')\n",
    "df = df[df[\"Buffett_statement\"] == 1]\n",
    "\n",
    "df = df.dropna(subset=[\"Question\", \"Answer\"])  # drop any incomplete rows\n",
    "print(f\"Filtered dataset size: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert pandas DataFrame to HF Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Optionally, split into train/validation\n",
    "# For a small dataset, you might do e.g. 90% train, 10% val\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\chewy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "model_name = \"AdaptLLM/finance-chat\"\n",
    "model_name = \"oopere/FinChat-XS\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n",
    "# ^ Use torch_dtype if model is half-precision or you have GPU memory constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff32cb8accaa462aac7bc96b30e0980b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b1b07711324b5f8b162396fce9c061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Build prompt text\n",
    "    texts = [\n",
    "        f\"Question: {q}\\nAnswer: {a}\"\n",
    "        for q, a in zip(examples[\"Question\"], examples[\"Answer\"])\n",
    "    ]\n",
    "    \n",
    "    # Tokenize in a causal manner\n",
    "    # Note: We do not separate 'input' vs 'label' because in CLM\n",
    "    # the model learns to predict every next token in the sequence.\n",
    "    # We'll rely on the standard LM masking in the collator.\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=512,  # adjust as needed\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False  # Because this is Causal LM, not Masked LM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chewy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\chewy\\AppData\\Local\\Temp\\ipykernel_3888\\901397049.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 1:13:39, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.549900</td>\n",
       "      <td>2.700572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=147, training_loss=2.6107002829207855, metrics={'train_runtime': 4422.5682, 'train_samples_per_second': 0.268, 'train_steps_per_second': 0.033, 'total_flos': 671726644617600.0, 'train_loss': 2.6107002829207855, 'epoch': 2.951898734177215})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"oopere-FinChat-XS\",\n",
    "    per_device_train_batch_size=1,  # adjust to fit GPU\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # accumulate grads to simulate bigger batch\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",  # or \"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved at oopere-FinChat-XS\n"
     ]
    }
   ],
   "source": [
    "# saving it\n",
    "output_dir = \"oopere-FinChat-XS\"\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved at {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffetAI = AutoModelForCausalLM.from_pretrained(\"oopere-FinChat-XS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Normal Response----------\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "Question: Do you have any investing tips?\n",
      "Answer:\n",
      "assistant\n",
      "Yes. Investing is the key to financial growth and wealth creation. It involves diversifying your portfolio across various asset classes such as stocks bonds etc.; building an emergency fund; saving for long-term goals like retirement savings; using tax strategies that maximize returns on investment; managing risk through diversification; maintaining discipline in oneâ€™s investments (e.g., avoid taking excessive risks); understanding Fusion of market conditions when making decisions about which assets will perform best during certain periods; avoiding bad timing with investment strategy or advice from others who may be mistaken; minimizing fees associated with brokerage accounts. Maintaining patience while conducting research also helps make more informed decisions about what types of funds/assets to invest into; it can help reduce stress related to decision time after researching has been completed. Lastly, always keep up with news pertaining to local markets & other global trends that affect stock prices and values. You should do some homework before buying anything new so you know everything beforehand. Always ensure there aren't hidden costs involved if needed â€“ they might cost more than expected but could save money later. Be sure not to overinvest at times because sometimes people get too caught up spending all their excess capital without planning for future expenses, taxes and emergencies. Make plans BEFORE starting out, this\n",
      "----------BuffetAI Response----------\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "Question: Do you have any investing tips?\n",
      "Answer:\n",
      "assistant\n",
      "I would say that the most important thing is to understand your risk tolerance and invest in stocks with high growth potential. It's also essential to diversify across different sectors, but not too much â€“ 10% of investments should be from one sector. You want to balance it out so there aren't two large baskets full of certain companies. Donâ€™t get caught up in trying to time the market; instead focus on buying quality businesses at fair prices over long periods of time. I recommend reading books like Value Investor or The Intelligent Investor for more information about this subject. Good luck!\n",
      "\n",
      "https://www.youtube.com/watch?v=sZUc forex&feature=related   https://www.youtube.com/watch?v=-zM6mh8DQ3o&feature=embr\n",
      "https://www.youtube.com/watch?v=4KfY9NjL-2g#t=57ms  \n",
      " http://www.investopedia.com/terms/p/portfoliomanagement.aspx   https://www.investopedia.com/terms/a/accountability.asp   \n",
      "http://www.investopedia.com/terms/b\n"
     ]
    }
   ],
   "source": [
    "# Let's load the trained model from the output dir if needed\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"adaptllm-finance-buffett\")\n",
    "\n",
    "# Directly use model.generate():\n",
    "def buffett_answer(question, max_new_tokens=256):\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]   \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            repetition_penalty=1.2,\n",
    "            eos_token_id=tokenizer.eos_token_id,  # clearly defined stop token\n",
    "            pad_token_id=tokenizer.eos_token_id   # to handle padding gracefully\n",
    "        )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(buffetAI.device)\n",
    "    with torch.no_grad():\n",
    "        buffet_output_ids = buffetAI.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            repetition_penalty=1.2,\n",
    "            eos_token_id=tokenizer.eos_token_id,  # clearly defined stop token\n",
    "            pad_token_id=tokenizer.eos_token_id   # to handle padding gracefully\n",
    "        )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True), tokenizer.decode(buffet_output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Try it\n",
    "test_q = \"How should I think about investing during a recession?\"\n",
    "test_q = \"Do you have any investing tips?\"\n",
    "response, buffet_response = buffett_answer(test_q)\n",
    "print(\"-\"* 10 + \"Normal Response\" + \"-\"*10)\n",
    "print(response)\n",
    "print(\"-\"* 10 + \"BuffetAI Response\" + \"-\"*10)\n",
    "print(buffet_response)\n",
    "# \"Question: How should I think about investing during a recession? \n",
    "#  Answer: I focus on companies with strong fundamentals...\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
