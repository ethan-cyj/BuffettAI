{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethan/Documents/GitHub/BuffettAI/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM\n",
    ")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"oopere-FinChat-XS\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"oopere/FinChat-XS\")\n",
    "buffetAI = AutoModelForCausalLM.from_pretrained(\"oopere-FinChat-XS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['The Investment Industry', 'Technology', 'Education', 'Gold'], dtype='object', name='Section')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/buffet_qna_finetune_ready.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "# Identify rare classes\n",
    "class_counts = df['Section'].value_counts()\n",
    "rare_classes = class_counts[class_counts < 11].index\n",
    "print(rare_classes)\n",
    "\n",
    "# Combine rare classes into 'Other'\n",
    "df['Section'] = df['Section'].replace(rare_classes, 'Other')\n",
    "\n",
    "# Update Section_code after modification\n",
    "df['Section'] = df['Section'].astype('category')\n",
    "df['Section_code'] = df['Section'].cat.codes\n",
    "\n",
    "df['Section'] = df['Section'].astype('category')\n",
    "df['Section_code'] = df['Section'].cat.codes\n",
    "\n",
    "# Split the dataset into training, validation, and test sets, proportionately by Sections column\n",
    "# This will ensure that each split has a similar distribution of the Sections column\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.05, stratify=df['Section_code'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['Section_code'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Normal Response----------\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "Question: Do you have any investing tips?\n",
      "Answer:\n",
      "assistant\n",
      "Investment advice can be obtained from various resources. The most common ones include the FINRA Investor Education Foundation and NerdWallet's Investing for Beginners section of their website. Other sources may also provide useful information on investment strategies that could help one achieve financial stability. \n",
      "\n",
      "It is important to consult with an expert such as your own financial advisor or someone who has experience in finance before making decisions about investments. It would not only benefit you but others too if everyone had access to reliable sources which advise accordingly without causing unnecessary harm. \n",
      "Additionally, there should always consider how much risk one wants to take on when deciding what type of stocks they want to invest in. One must ensure it doesnt compromise future goals based solely off this single factor at hand. Always keep up-to-date knowledge so you will stay ahead of trends hence being able to adapt wisely.  \n",
      "Remember that no matter how well planned something goes wrong sometimes happens; therefore we must learn from mistakes and move forward every time! This way you won't regret anything once again while trying to make smart choices through these steps. Be sure to review regularly whether things go as expected or differently than initially thought.\n",
      "I recommend consulting your local stockbroker or using online tools like\n",
      "----------BuffetAI Response----------\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "Question: Do you have any investing tips?\n",
      "Answer:\n",
      "assistant\n",
      "No investment advice - I don't know what's in your head. You should always do your own research and be smart about it. Don’t buy anything that doesn’t make sense to you; if something sounds too good to pass up – but is just an idea or hearsay from someone else – then run away fast! It may sound like common-sense wisdom today but tomorrow it will not work out as expected. Always evaluate things before making money decisions. And never sell because the price goes down. Just keep buying more until you get back on top of everything. Enjoy life – there isn’t much upside after 50. If you start doing this at age 21, you can live forever. The universe has given us enough for our time here so why would we want to spend all of it chasing money when we could enjoy being alive instead? There is no such thing as a bad stock market day. Every great business turns into one big deal every now and again. All businesses need some upsides over their lifetime, though they usually won’t last long. This applies to both stocks and bonds (and even real estate). Investments are going to happen anyway, so focus on finding those opportunities where you really believe you\n"
     ]
    }
   ],
   "source": [
    "def buffett_answer(question, max_new_tokens=256):\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]   \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            repetition_penalty=1.2,\n",
    "            eos_token_id=tokenizer.eos_token_id,  # clearly defined stop token\n",
    "            pad_token_id=tokenizer.eos_token_id   # to handle padding gracefully\n",
    "        )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(buffetAI.device)\n",
    "    with torch.no_grad():\n",
    "        buffet_output_ids = buffetAI.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            repetition_penalty=1.2,\n",
    "            eos_token_id=tokenizer.eos_token_id,  # clearly defined stop token\n",
    "            pad_token_id=tokenizer.eos_token_id   # to handle padding gracefully\n",
    "        )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True), tokenizer.decode(buffet_output_ids[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "base_preds = []\n",
    "buffet_preds = []\n",
    "references = []\n",
    "\n",
    "for i, row in test_df.iterrows():\n",
    "    question = row['Questions']\n",
    "    reference = row['Answers']\n",
    "\n",
    "    base_resp, buffet_resp = buffett_answer(question)\n",
    "    \n",
    "    # Append generated and reference responses\n",
    "    base_preds.append(base_resp.replace(f\"Question: {question}\\nAnswer:\", \"\").strip())\n",
    "    buffet_preds.append(buffet_resp.replace(f\"Question: {question}\\nAnswer:\", \"\").strip())\n",
    "    references.append([reference.strip()])  # BLEU expects list of references per prediction\n",
    "\n",
    "#save the preds\n",
    "base_preds_df = pd.DataFrame(base_preds, columns=[\"Base_Predictions\"])\n",
    "buffet_preds_df = pd.DataFrame(buffet_preds, columns=[\"Buffet_Predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prompt(text):\n",
    "    # Split on 'assistant' and take the part after\n",
    "    parts = text.split(\"assistant\\n\", 1)\n",
    "    return parts[1].strip() if len(parts) > 1 else text.strip()\n",
    "\n",
    "# Apply to both prediction DataFrames\n",
    "base_preds_df['Base_Predictions'] = base_preds_df['Base_Predictions'].apply(clean_prompt)\n",
    "buffet_preds_df['Buffet_Predictions'] = buffet_preds_df['Buffet_Predictions'].apply(clean_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the preds\n",
    "base_preds_df.to_csv('base_preds.csv', index=False)\n",
    "buffet_preds_df.to_csv('buffet_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model BLEU Score: 0.0065743126011524496\n",
      "BuffetAI BLEU Score: 0.006237590236604703\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "\n",
    "# Compute BLEU scores\n",
    "base_bleu = bleu.compute(predictions=base_preds, references=references)\n",
    "buffet_bleu = bleu.compute(predictions=buffet_preds, references=references)\n",
    "\n",
    "# Show results\n",
    "print(\"Base Model BLEU Score:\", base_bleu[\"bleu\"])\n",
    "print(\"BuffetAI BLEU Score:\", buffet_bleu[\"bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import dotenv\n",
    "from groq import Groq  # Import the Groq API client library\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "# Initialize the Groq client using your API key.\n",
    "client = Groq(api_key=os.environ.get(\"GROQAPIKEY\"))\n",
    "# Endpoints for the Ollama and baseline model API calls.\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "BASELINE_API_URL = \"http://localhost:11434/api/generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ollama(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Directly calls the finetuned local Ollama model endpoint.\n",
    "    \n",
    "    The query is combined with a Warren Buffett persona instruction\n",
    "    without any extra documents appended.\n",
    "    \"\"\"\n",
    "    # Create the prompt by prepending the persona instruction.\n",
    "    formatted_prompt = (\n",
    "        \"You are Warren Buffett, the CEO of Berkshire Hathaway. \" +\n",
    "        query\n",
    "    )\n",
    "    data = {\n",
    "        \"model\": \"ollama_buffett\",  # Name of your finetuned local model.\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json=data)\n",
    "        response.raise_for_status()\n",
    "        json_response = response.json()\n",
    "        return json_response.get(\"response\", \"\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error calling Ollama: {e}\")\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "def call_baseline(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Directly calls a baseline model endpoint for response generation.\n",
    "    \n",
    "    The query is combined with a similar Warren Buffett instruction but\n",
    "    without any appended documents.\n",
    "    \"\"\"\n",
    "    formatted_prompt = (\n",
    "        \"You are Warren Buffett, the CEO of Berkshire Hathaway. \" +\n",
    "        query\n",
    "    )\n",
    "    data = {\n",
    "        \"model\": \"ollama_baseline\",  # Identifier for the baseline model.\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(BASELINE_API_URL, json=data)\n",
    "        response.raise_for_status()\n",
    "        json_response = response.json()\n",
    "        return json_response.get(\"response\", \"\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error calling baseline model: {e}\")\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "def evaluate_response_groq(query: str, response_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates a generated response using the Groq API.\n",
    "    \n",
    "    The evaluation prompt asks the LLM judge to assign scores on:\n",
    "      1. Query Relevance (0-3)\n",
    "      2. Data Accuracy (0-3)\n",
    "      3. Clarity (0-3)\n",
    "      4. Buffet-Likeness (0-5)\n",
    "    \n",
    "    The overall_score is the average of these scores.\n",
    "    \"\"\"\n",
    "    evaluation_prompt = f\"\"\"\n",
    "\tEvaluate the following response based on these criteria:\n",
    "\n",
    "\t1. Query Relevance (0-3): How well does the response answer the query: \"{query}\"?\n",
    "\t2. Data Accuracy (0-3): How factually correct is the response?\n",
    "\t3. Clarity (0-3): How clear and easy to understand is the response?\n",
    "\t4. Buffet-Likeness (0-5): How well does the response capture Warren Buffett’s style, wisdom, and personality?\n",
    "\n",
    "\tReturn your evaluation as a JSON object with the following keys:\n",
    "\t\"query_relevance\", \"data_accuracy\", \"clarity\", \"buffet_likeness\", and \"overall_score\" (average of the above scores).\n",
    "\n",
    "\tResponse to evaluate:\n",
    "\t{response_text}\n",
    "\t\"\"\".strip()\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"llama-3.3-70b-versatile\",  # Use the Groq model as specified in your reference.\n",
    "            temperature=0.1,\n",
    "            max_tokens=70\n",
    "            \n",
    "        )\n",
    "        # The API returns a response object. Here we assume the evaluation result is contained \n",
    "        # in chat_completion.choices[0].message.content as a JSON string.\n",
    "        result_content = chat_completion.choices[0].message.content\n",
    "        print(f\"Evaluation result: {result_content}\")\n",
    "        return json.loads(result_content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation with Groq: {e}\")\n",
    "        return {\"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Who is Warren Buffett and what is your investment philosophy?\n",
      "------------------------------------------------------------\n",
      "Ollama Response:\n",
      "Hi! I'm Charlie, founder of CNET's sister site Kotaku.com. Can you help me find a good board game or two for my new friend here (referring to an older person in the audience)? And why do people like [Buffett] so much? He doesn't really speak to that - it is just always interesting who gets all of the accolades and when they get them.\n",
      "\n",
      "Evaluation result: ```json\n",
      "{\n",
      "  \"query_relevance\": 0,\n",
      "  \"data_accuracy\": 0,\n",
      "  \"clarity\": 2,\n",
      "  \"buffet_likeness\": 0,\n",
      "  \"overall_score\": 0.4\n",
      "}\n",
      "```\n",
      "\n",
      "Explanation:\n",
      "- Query Relevance: 0 (The response does not address the query\n",
      "Error during evaluation with Groq: Expecting value: line 1 column 1 (char 0)\n",
      "Ollama Evaluation:\n",
      "{\n",
      "  \"error\": \"Expecting value: line 1 column 1 (char 0)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "\t\"Who is Warren Buffett and what is your investment philosophy?\",\n",
    "\t# \"Tell me about the latest trades of Berkshire Hathaway.\",\n",
    "\t# \"How do you view the current market conditions?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "\tprint(f\"\\nQuery: {query}\\n{'-' * 60}\")\n",
    "\t\n",
    "\t# Get response from the finetuned local Ollama model.\n",
    "\tollama_response = call_ollama(query)\n",
    "\tprint(\"Ollama Response:\")\n",
    "\tprint(ollama_response)\n",
    "\t\n",
    "\t# Evaluate the Ollama response.\n",
    "\tollama_eval = evaluate_response_groq(query, ollama_response)\n",
    "\tprint(\"Ollama Evaluation:\")\n",
    "\tprint(json.dumps(ollama_eval, indent=2))\n",
    "\t\n",
    "\t# # Get response from the baseline model.\n",
    "\t# baseline_response = call_baseline(query)\n",
    "\t# print(\"Baseline Response:\")\n",
    "\t# print(baseline_response)\n",
    "\t\n",
    "\t# # Evaluate the baseline response.\n",
    "\t# baseline_eval = evaluate_response_groq(query, baseline_response)\n",
    "\t# print(\"Baseline Evaluation:\")\n",
    "\t# print(json.dumps(baseline_eval, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result: ```json\n",
      "{\n",
      "  \"query_relevance\": 3,\n",
      "  \"data_accuracy\": 3,\n",
      "  \"clarity\": 3,\n",
      "  \"buffet_likeness\": 4,\n",
      "  \"overall_score\": 3.25\n",
      "}\n",
      "```\n",
      "\n",
      "Explanation:\n",
      "- Query Relevance: 3 (The response directly answers the query,\n",
      "Error during evaluation with Groq: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'error': 'Expecting value: line 1 column 1 (char 0)'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_response_groq(\n",
    "\t\"What is the most important lesson you have learned in your investment career?\",\n",
    "\t\"The most important lesson I have learned in my investment career is the value of patience and long-term thinking. Successful investing requires a deep understanding of businesses, their fundamentals, and the ability to hold onto investments through market fluctuations. It's essential to focus on the intrinsic value of a company rather than short-term market trends.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
