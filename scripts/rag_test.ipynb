{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain_core.documents import Document\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "def safe_load_pickle(file_path: str) -> Any:\n",
    "    \"\"\"Safely load pickle files that may contain pandas objects\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        try:\n",
    "            # First try standard pickle load\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        except (TypeError, AttributeError, pickle.UnpicklingError):\n",
    "            try:\n",
    "                # Fallback to pandas read_pickle\n",
    "                return pd.read_pickle(file_path)\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Failed to load {file_path}: {str(e)}\")\n",
    "\n",
    "def inspect_file(file_path: str) -> None:\n",
    "    \"\"\"Debug function to examine file structure\"\"\"\n",
    "    print(f\"\\n=== Inspecting {file_path} ===\")\n",
    "    try:\n",
    "        if file_path.endswith(('.xlsx', '.xls')):\n",
    "            df = pd.read_excel(file_path)\n",
    "            print(\"Excel file detected\")\n",
    "            print(f\"Shape: {df.shape}\")\n",
    "            print(\"Columns:\", df.columns.tolist())\n",
    "            print(\"\\nFirst row:\")\n",
    "            pprint(df.iloc[0].to_dict())\n",
    "        elif file_path.endswith('.pkl'):\n",
    "            data = safe_load_pickle(file_path)\n",
    "            print(f\"Type: {type(data)}\")\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                print(\"Pandas DataFrame detected\")\n",
    "                print(f\"Shape: {data.shape}\")\n",
    "                print(\"Columns:\", data.columns.tolist())\n",
    "                print(\"\\nFirst row:\")\n",
    "                pprint(data.iloc[0].to_dict())\n",
    "            elif isinstance(data, list):\n",
    "                print(f\"List of {len(data)} items\")\n",
    "                if data:\n",
    "                    print(\"\\nFirst item type:\", type(data[0]))\n",
    "                    if isinstance(data[0], dict):\n",
    "                        print(\"Keys in first item:\", data[0].keys())\n",
    "            elif isinstance(data, dict):\n",
    "                print(\"Dictionary with keys:\", data.keys())\n",
    "            else:\n",
    "                print(\"Content sample:\", str(data)[:200] + \"...\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format\")\n",
    "    except Exception as e:\n",
    "        print(f\"Inspection failed: {str(e)}\")\n",
    "\n",
    "def load_buffet_qna_xlsx(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load Q&A data from Excel with columns: Section, Questions, Answers\n",
    "    Returns List[Document] where:\n",
    "    - page_content contains formatted Q&A\n",
    "    - metadata contains structured fields\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_cols = {'Section', 'Questions', 'Answers'}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in Q&A file: {missing}\")\n",
    "\n",
    "    documents = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Create human-readable content\n",
    "        page_content = (\n",
    "            f\"Question: {row['Questions']}\\n\"\n",
    "            f\"Answer: {row['Answers']}\\n\"\n",
    "            f\"Section: {row['Section']}\"\n",
    "        )\n",
    "        \n",
    "        # Store structured data in metadata\n",
    "        metadata = {\n",
    "            \"section\": row[\"Section\"],\n",
    "            \"question\": row[\"Questions\"],\n",
    "            \"answer\": row[\"Answers\"],\n",
    "            \"source\": \"buffet_qna\"\n",
    "        }\n",
    "        \n",
    "        documents.append(Document(\n",
    "            page_content=page_content,\n",
    "            metadata=metadata\n",
    "        ))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def load_brka_trades_xlsx(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load trades data from Excel with financial columns\n",
    "    Returns List[Document] where:\n",
    "    - page_content contains key trade info\n",
    "    - metadata contains all raw data\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_cols = {'RIC', 'Security Name', 'Date', 'Position', 'Position Change'}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in trades file: {missing}\")\n",
    "\n",
    "    documents = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Create human-readable summary\n",
    "        page_content = (\n",
    "            f\"Security: {row['Security Name']} ({row['RIC']})\\n\"\n",
    "            f\"Date: {row['Date']}\\n\"\n",
    "            f\"Position: {row['Position']:,} shares\\n\"\n",
    "            f\"Change: {row['Position Change']:+,}\"\n",
    "        )\n",
    "        \n",
    "        # Store all raw data in metadata\n",
    "        metadata = row.to_dict()\n",
    "        metadata.update({\"source\": \"brka_trades\"})\n",
    "        \n",
    "        documents.append(Document(\n",
    "            page_content=page_content,\n",
    "            metadata=metadata\n",
    "        ))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def load_news_pickle(file_path: str) -> List[Document]:\n",
    "    \"\"\"Load news pickle with flexible format handling\"\"\"\n",
    "    data = safe_load_pickle(file_path)\n",
    "    \n",
    "    # Handle DataFrame case\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = data.to_dict('records')\n",
    "    \n",
    "    # Handle list of dicts\n",
    "    if isinstance(data, list) and all(isinstance(x, dict) for x in data):\n",
    "        return [\n",
    "            Document(\n",
    "                page_content=item.get('text', item.get('content', str(item))),\n",
    "                metadata={k: v for k, v in item.items() \n",
    "                         if k not in ['text', 'content']}\n",
    "            )\n",
    "            for item in data\n",
    "        ]\n",
    "    \n",
    "    # Handle single dictionary\n",
    "    elif isinstance(data, dict):\n",
    "        return [Document(\n",
    "            page_content=data.get('text', data.get('content', str(data))),\n",
    "            metadata={k: v for k, v in data.items() \n",
    "                     if k not in ['text', 'content']}\n",
    "        )]\n",
    "    \n",
    "    # Fallback for other formats\n",
    "    return [Document(page_content=str(data))]\n",
    "\n",
    "def load_shareholder_letters_pickle(file_path: str) -> List[Document]:\n",
    "    \"\"\"Load shareholder letters with year-based structure\"\"\"\n",
    "    data = safe_load_pickle(file_path)\n",
    "    \n",
    "    if isinstance(data, dict) and all(isinstance(k, (str, int)) for k in data):\n",
    "        return [\n",
    "            Document(\n",
    "                page_content=content,\n",
    "                metadata={\"year\": year, \"source\": \"shareholder_letter\"}\n",
    "            )\n",
    "            for year, content in data.items()\n",
    "        ]\n",
    "    \n",
    "    # Fallback for other formats\n",
    "    return load_news_pickle(file_path)\n",
    "\n",
    "def load_documents(file_path: str) -> List[Document]:\n",
    "    \"\"\"Main document loading interface\"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.xlsx'):\n",
    "            if 'buffet_qna' in file_path.lower():\n",
    "                return load_buffet_qna_xlsx(file_path)\n",
    "            elif 'brka_trades' in file_path.lower():\n",
    "                return load_brka_trades_xlsx(file_path)\n",
    "        \n",
    "        elif file_path.endswith('.pkl'):\n",
    "            if 'news' in file_path.lower():\n",
    "                return load_news_pickle(file_path)\n",
    "            elif 'shareholder' in file_path.lower():\n",
    "                return load_shareholder_letters_pickle(file_path)\n",
    "        \n",
    "        raise ValueError(f\"Unrecognized file type: {file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return [Document(\n",
    "            page_content=f\"Error loading document: {str(e)}\",\n",
    "            metadata={\"source\": \"error\", \"file_path\": file_path}\n",
    "        )]\n",
    "\n",
    "def load_all_documents(\n",
    "    base_path: str = \"/data\",\n",
    "    buffet_qna_path: Optional[str] = None,\n",
    "    brka_trades_path: Optional[str] = None,\n",
    "    news_path: Optional[str] = None,\n",
    "    shareholder_letters_path: Optional[str] = None,\n",
    "    verbose: bool = True\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load all 4 document sources at once.\n",
    "    \n",
    "    Args:\n",
    "        base_path: Base directory for files if individual paths not specified\n",
    "        *_path: Override individual file paths\n",
    "        verbose: Print loading progress\n",
    "    \n",
    "    Returns:\n",
    "        Combined list of Documents from all sources\n",
    "    \"\"\"\n",
    "    # Set default paths if not specified\n",
    "    buffet_qna_path = buffet_qna_path or Path(base_path) / \"buffet_qna.xlsx\"\n",
    "    brka_trades_path = brka_trades_path or Path(base_path) / \"brka_trades.xlsx\"\n",
    "    news_path = news_path or Path(base_path) / \"news.pkl\"\n",
    "    shareholder_letters_path = shareholder_letters_path or Path(base_path) / \"shareholder_letters.pkl\"\n",
    "    \n",
    "    all_docs = []\n",
    "    \n",
    "    # Load each file with progress reporting\n",
    "    for file_path, loader in [\n",
    "        (buffet_qna_path, load_buffet_qna_xlsx),\n",
    "        (brka_trades_path, load_brka_trades_xlsx),\n",
    "        (news_path, load_news_pickle),\n",
    "        (shareholder_letters_path, load_shareholder_letters_pickle)\n",
    "    ]:\n",
    "        try:\n",
    "            if verbose:\n",
    "                print(f\"Loading {file_path}...\")\n",
    "            docs = loader(file_path)\n",
    "            all_docs.extend(docs)\n",
    "            if verbose:\n",
    "                print(f\"Loaded {len(docs)} documents\")\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Error loading {file_path}: {str(e)}\")\n",
    "            all_docs.append(Document(\n",
    "                page_content=f\"Error loading {file_path}: {str(e)}\",\n",
    "                metadata={\"source\": \"error\", \"file_path\": str(file_path)}\n",
    "            ))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nTotal documents loaded: {len(all_docs)}\")\n",
    "    \n",
    "    return all_docs\n",
    "\n",
    "# print(f\"Script running from: {os.getcwd()}\")\n",
    "\n",
    "# print(load_documents(\"../data/news.pkl\"))\n",
    "# print(load_all_documents(\"../data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "CHUNK_SIZE = 1024\n",
    "CHUNK_OVERLAP = 20\n",
    "\n",
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Splits a list of document objects into manageable text chunks.\n",
    "    Assumes each document has a 'content' field.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    # For compatibility with langchain's splitter, wrap your dicts in a simple object:\n",
    "    # Here we assume each document is a dict with a 'content' key.\n",
    "    # You might need to convert these dicts to the Document type expected by langchain.\n",
    "    return splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "# Compare 2 different retrieval methods and evaluate\n",
    "\n",
    "def build_bm25(corpus):\n",
    "    \"\"\"\n",
    "    Builds a BM25 model from a list of texts (corpus).\n",
    "    \"\"\"\n",
    "    return BM25Okapi(corpus)\n",
    "\n",
    "def build_faiss_index(documents):\n",
    "    \"\"\"\n",
    "    Uses OpenAI embeddings to build a FAISS vectorstore from documents.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    # FAISS vectorstore will build the index based on document 'content'\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "def create_ensemble_retriever(bm25_retriever, faiss_retriever):\n",
    "    \"\"\"\n",
    "    Combines BM25 and FAISS retrievers into an ensemble retriever.\n",
    "    \"\"\"\n",
    "    return EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import CrossEncoder\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the pre-trained cross-encoder (adjust model name as needed)\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_documents(query: str, documents: List[Document], top_k: int = 5) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Reranks documents using a cross-encoder model.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        documents: List of LangChain Document objects to rerank\n",
    "        top_k: Number of top documents to return\n",
    "        \n",
    "    Returns:\n",
    "        List of reranked Document objects\n",
    "    \"\"\"\n",
    "    # Create pairs of (query, document_content)\n",
    "    pairs = [[query, doc.page_content] for doc in documents]  # Changed to use page_content\n",
    "    \n",
    "    # Get scores from cross-encoder\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Sort by scores in descending order and get top_k indices\n",
    "    ranked_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    \n",
    "    # Return documents in ranked order\n",
    "    return [documents[i] for i in ranked_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def create_prompt(query: str, company_name: str, documents: List[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Creates a focused prompt for the LLM using both the query and retrieved documents.\n",
    "    \n",
    "    Args:\n",
    "        query: The original user query/search terms\n",
    "        company_name: Target company for the report\n",
    "        documents: List of retrieved LangChain Document objects\n",
    "        \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    context = \"\\n\".join([f\"Source {i+1}:\\n{doc.page_content}\\n\" \n",
    "                        for i, doc in enumerate(documents)])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "**Task**: Generate a comprehensive investment report for {company_name} that specifically addresses: \n",
    "\"{query}\"\n",
    "\n",
    "**Context from company documents**:\n",
    "{context}\n",
    "\n",
    "**Report Requirements**:\n",
    "1. Directly respond to the query about {query}\n",
    "2. Focus on these key aspects (if relevant):\n",
    "   - Revenue growth trends and drivers\n",
    "   - Profitability metrics and margins\n",
    "   - Strategic initiatives and investments\n",
    "   - Competitive positioning\n",
    "   - Risks and challenges\n",
    "3. Include specific numbers, percentages, and timeframes when available\n",
    "4. Highlight any contradictions or uncertainties in the data\n",
    "5. Maintain objective, professional tone\n",
    "\n",
    "**Report Structure**:\n",
    "[Overview] - Brief introduction addressing the query\n",
    "[Key Findings] - Bullet points of most relevant insights\n",
    "[Detailed Analysis] - Expanded discussion with supporting data\n",
    "[Conclusions] - Summary and forward-looking statements\n",
    "\n",
    "**Begin Report for {company_name}**:\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "def create_evaluation_prompt(query: str, generated_report: str, \n",
    "                           retrieved_documents: List[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Creates an evaluation prompt that assesses how well the report addresses the query.\n",
    "    \n",
    "    Args:\n",
    "        query: Original user question/search terms\n",
    "        generated_report: LLM-generated report to evaluate\n",
    "        retrieved_documents: Documents used for context\n",
    "        \n",
    "    Returns:\n",
    "        Formatted evaluation prompt\n",
    "    \"\"\"\n",
    "    context_samples = \"\\n\".join([f\"• {doc.page_content[:200]}...\" \n",
    "                               for doc in retrieved_documents[:3]])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "**Evaluation Task**: Assess how well this investment report addresses the original query.\n",
    "\n",
    "**Original Query**: \"{query}\"\n",
    "\n",
    "**Generated Report**:\n",
    "{generated_report}\n",
    "\n",
    "**Sample Supporting Context**:\n",
    "{context_samples}\n",
    "\n",
    "**Evaluation Rubric**:\n",
    "1. Query Relevance (0-10): \n",
    "   - Does every section directly address aspects of \"{query}\"?\n",
    "   - Are there any irrelevant tangents?\n",
    "\n",
    "2. Data Accuracy (0-10):\n",
    "   - Are all facts supported by the context documents?\n",
    "   - Are numbers and claims properly qualified?\n",
    "\n",
    "3. Analytical Depth (0-10):\n",
    "   - Does it surface non-obvious insights from the data?\n",
    "   - Does it identify relationships between different metrics?\n",
    "\n",
    "4. Actionability (0-10):\n",
    "   - Would an investor find clear takeaways?\n",
    "   - Are risks and opportunities properly highlighted?\n",
    "\n",
    "**Output Format**:\n",
    "- For each criterion: \n",
    "  [Score] [Justification in 1-2 sentences]\n",
    "- Overall summary (1 paragraph)\n",
    "\n",
    "**Begin Evaluation**:\n",
    "\"\"\"\n",
    "    return prompt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai \n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "from langchain_core.documents import Document\n",
    "from rag.text_splitter import split_documents\n",
    "from rag.retrieval import build_bm25, build_faiss_index, create_ensemble_retriever\n",
    "from rag.reranker import rerank_documents\n",
    "from rag.prompt_engineering import create_prompt, create_evaluation_prompt\n",
    "\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# load env variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# set OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"API key is not set\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(\n",
    "        self, \n",
    "        raw_documents,\n",
    "        llm_type: str = \"openai\",  # \"openai\" or \"custom\"\n",
    "        custom_llm: Optional[Callable] = None,\n",
    "        openai_model: str = \"gpt-3.5-turbo\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize RAG pipeline with LLM options.\n",
    "        \n",
    "        Args:\n",
    "            raw_documents: Input documents for retrieval\n",
    "            llm_type: \"openai\" or \"custom\"\n",
    "            custom_llm: Function(prompt: str, context: str) -> str\n",
    "            openai_model: OpenAI model name if using OpenAI\n",
    "        \"\"\"\n",
    "        self.documents = split_documents(raw_documents)\n",
    "        self.context_memory = []\n",
    "        self.llm_type = llm_type\n",
    "        self.custom_llm = custom_llm\n",
    "        self.openai_model = openai_model\n",
    "        \n",
    "        if llm_type == \"openai\":\n",
    "            import openai\n",
    "            openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not openai.api_key:\n",
    "                raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "        \n",
    "        self._initialize_retrievers()\n",
    "\n",
    "    def _initialize_retrievers(self):\n",
    "        \"\"\"Initialize BM25 and FAISS retrievers\"\"\"\n",
    "        # BM25 Retriever\n",
    "        from langchain.retrievers import BM25Retriever\n",
    "        self.bm25_retriever = BM25Retriever.from_documents(self.documents)\n",
    "        self.bm25_retriever.k = 5\n",
    "        \n",
    "        # FAISS Retriever\n",
    "        self.faiss_retriever = build_faiss_index(self.documents).as_retriever(search_kwargs={\"k\": 5})\n",
    "        \n",
    "        # Ensemble Retriever\n",
    "        self.ensemble_retriever = create_ensemble_retriever(\n",
    "            self.bm25_retriever, \n",
    "            self.faiss_retriever\n",
    "        )\n",
    "    \n",
    "    def generate_text(self, prompt: str, context: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        Unified text generation interface.\n",
    "        Routes to OpenAI or custom LLM based on configuration.\n",
    "        \"\"\"\n",
    "        if self.llm_type == \"custom\" and self.custom_llm:\n",
    "            return self.custom_llm(prompt, context)\n",
    "        elif self.llm_type == \"openai\":\n",
    "            return self._call_openai(prompt, context)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid LLM configuration\")\n",
    "\n",
    "    def _call_openai(self, prompt: str, context: str = \"\") -> str:\n",
    "        \"\"\"Internal method for OpenAI API calls\"\"\"\n",
    "        try:\n",
    "            import openai\n",
    "            messages = []\n",
    "            if context:\n",
    "                messages.append({\"role\": \"system\", \"content\": context})\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.openai_model,\n",
    "                messages=messages,\n",
    "                temperature=0.7,\n",
    "                max_tokens=1500\n",
    "            )\n",
    "            return response.choices[0].message['content']\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenAI: {e}\")\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "    def retrieve_and_rerank(self, query: str, retriever, top_k: int = 5) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Retrieves and reranks documents.\n",
    "        \"\"\"\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        return rerank_documents(query, docs, top_k=top_k)\n",
    "\n",
    "    def generate_report(self, query: str, company_name: str, retriever) -> tuple:\n",
    "        \"\"\"\n",
    "        Generates a report using retrieved documents.\n",
    "        \"\"\"\n",
    "        retrieved_docs = self.retrieve_and_rerank(query, retriever)\n",
    "        prompt = create_prompt(query, company_name, retrieved_docs)\n",
    "        context_text = \"\\n\".join(self.context_memory)\n",
    "        \n",
    "        report = self.generate_text(prompt, context=context_text)\n",
    "        \n",
    "        # Update context memory\n",
    "        self.context_memory.append(f\"Query: {query}\")\n",
    "        self.context_memory.append(f\"Report: {report[:200]}...\")\n",
    "        \n",
    "        return report, retrieved_docs\n",
    "\n",
    "    def evaluate_report(self, query: str, report: str, retrieved_docs: List[Document]) -> str:\n",
    "        \"\"\"\n",
    "        Evaluates the generated report.\n",
    "        \"\"\"\n",
    "        eval_prompt = create_evaluation_prompt(query, report, retrieved_docs)\n",
    "        return self.generate_text(eval_prompt)\n",
    "\n",
    "    def process_query(self, query: str, company_name: str, method: str = \"ensemble\") -> tuple:\n",
    "        \"\"\"\n",
    "        Main method to process a query through the RAG pipeline.\n",
    "        \"\"\"\n",
    "        retriever = {\n",
    "            \"bm25\": self.bm25_retriever,\n",
    "            \"faiss\": self.faiss_retriever,\n",
    "            \"ensemble\": self.ensemble_retriever\n",
    "        }.get(method, self.ensemble_retriever)\n",
    "        \n",
    "        report, retrieved_docs = self.generate_report(query, company_name, retriever)\n",
    "        evaluation = self.evaluate_report(query, report, retrieved_docs)\n",
    "        \n",
    "        return report, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document loading\n",
    "if args.load_all:\n",
    "    print(\"Loading all standard documents from data/ directory...\")\n",
    "    raw_docs = load_all_documents(base_path=\"../data\")\n",
    "else:\n",
    "    raw_docs = load_documents(args.data_path)\n",
    "\n",
    "# Initialize and run pipeline\n",
    "pipeline = RAGPipeline(raw_docs)\n",
    "report, evaluation = pipeline.process_query(\n",
    "    args.query, args.company, method=args.method\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
